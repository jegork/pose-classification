{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install scikit-learn -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import os\n",
    "import numpy as np\n",
    "from operator import itemgetter \n",
    "from random import shuffle \n",
    "import optuna\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = []\n",
    "y_list = []\n",
    "\n",
    "for class_n, c in enumerate(['kiss', 'handshake', 'hug', 'highfive']):\n",
    "    f = os.listdir('output/'+c)\n",
    "    f = list(map(lambda x: 'output/'+c+'/'+x, f))\n",
    "    \n",
    "    filtered = list(filter(lambda x: x.split('.')[-1] == 'npy', f))\n",
    "    \n",
    "    y_list.extend([class_n]*len(filtered))\n",
    "    files.extend(filtered)\n",
    "\n",
    "y = np.array(y_list)\n",
    "\n",
    "x = [np.load(x)/255 for x in files]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split(X, y, test_size=0.05):\n",
    "    shuffled_idx = np.random.permutation(len(X))\n",
    "    \n",
    "    split_at = int(len(shuffled_idx) * test_size)\n",
    "    train_idx = shuffled_idx[split_at:]\n",
    "    test_idx = shuffled_idx[:split_at]\n",
    "    \n",
    "    train_X = itemgetter(*train_idx)(X)\n",
    "    train_y = y.take(train_idx, 0)\n",
    "    \n",
    "    test_X = itemgetter(*test_idx)(X)\n",
    "    test_y = y.take(test_idx, 0)\n",
    "    \n",
    "    return (train_X, test_X, train_y, test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def onehot(y):\n",
    "    onehot = np.zeros((y.size, 4))\n",
    "    onehot[np.arange(y.size),y] = 1\n",
    "    return onehot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = onehot(y)\n",
    "train_X, test_X, train_y, test_y = split(x, y, 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 50\n",
    "\n",
    "def batcherize(x, y):\n",
    "    df = list(zip(x, y))\n",
    "    df = df * epochs\n",
    "    \n",
    "    for features, labels in df:\n",
    "        yield np.array([features]), np.array([labels])\n",
    "        \n",
    "def batcherize_t(x, y):\n",
    "    for features, labels in zip(x, y):\n",
    "        yield np.array([features]), np.array([labels])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = tf.data.Dataset.from_generator(batcherize_t, output_types=(tf.float64, tf.float64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(InputLayer((None, 128, 128,3)))\n",
    "model.add(TimeDistributed(\n",
    "    Conv2D(\n",
    "        filters=4, \n",
    "        kernel_size=4, \n",
    "        input_shape=(128, 128, 3), \n",
    "        data_format='channels_last', \n",
    "        padding=\"same\",\n",
    "        kernel_initializer='he_normal',\n",
    "    ), \n",
    "    name='Conv2D'))\n",
    "model.add(TimeDistributed(MaxPooling2D(pool_size=4), name='MaxPooling2D'))\n",
    "model.add(TimeDistributed(Flatten(), name='Flatten'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(LSTM(16))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(8, kernel_regularizer=tf.keras.regularizers.L2(0.4)))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(4, activation='softmax'))\n",
    "\n",
    "\n",
    "model.compile(optimizer=tf.keras.optimizers.RMSprop(), loss='cosine_similarity', metrics=['accuracy'])\n",
    "\n",
    "batcherize_train = batcherize(train_X, train_y)\n",
    "batcherize_test = batcherize(test_X, test_y)\n",
    "\n",
    "model.fit(\n",
    "    batcherize_train,\n",
    "    batch_size=1, \n",
    "    epochs=epochs, \n",
    "    steps_per_epoch=180,\n",
    "    validation_data=batcherize_test,\n",
    "    validation_steps=20,\n",
    ")"
   ]
  }
 ]
}