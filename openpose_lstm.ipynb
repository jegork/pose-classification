{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "source": [
    "# Data loading"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = ['hug', 'kiss', 'highfive', 'handshake']\n",
    "csv_list = os.listdir('out/handshake')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_videos = {}\n",
    "total = []\n",
    "\n",
    "factorize_classes = {_class: key for (_class, key) in zip(classes, range(len(classes)))}\n",
    "\n",
    "idx = 0\n",
    "\n",
    "for _class in classes:\n",
    "    if os.path.isdir('out/'+_class):\n",
    "        class_videos[_class] = os.listdir('out/'+_class)\n",
    "\n",
    "for _class, files in class_videos.items():\n",
    "    for file in files:\n",
    "        if os.path.isfile(\"out/handshake/\"+file):\n",
    "            if file.split('.')[-1] == 'csv':\n",
    "                csv = pd.read_csv(\"out/handshake/\"+file)\n",
    "                \n",
    "                file_id = int(file.split('.')[0].lstrip('0'))\n",
    "                video_number = pd.DataFrame({'_id': idx, 'video':[file_id]*csv.shape[0]})\n",
    "\n",
    "                csv['result'] = factorize_classes[_class]\n",
    "                total.append(pd.concat([video_number, csv], axis=1))\n",
    "                idx += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = pd.concat(total, ignore_index=True)"
   ]
  },
  {
   "source": [
    "# Data preprocessing"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_columns = list(filter(lambda x: x.endswith('score'), list(result.columns)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = result.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.iloc[:, 4:-1] = result.iloc[:, 4:-1].replace(0, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(result.iloc[:, 4:-1] == 0).sum().sum() / result.iloc[:, 4:-1].size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Proportion of NAs cells in dataset: ' + str((result.iloc[:, 4:-1] == -1).sum().sum() / result.iloc[:, 4:-1].size))\n",
    "print('Proportion of NAs rows in dataset: ' + str((result.iloc[:, 4:-1] == -1).sum(1).count() / result.iloc[:, 4:-1].shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(result[score_columns] == -1).sum().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# columns with parts of lower body which might poorly contribute to prediction of interactions\n",
    "to_drop = list(filter(lambda x: x.find('Ankle') != -1 or x.find('Hip') != -1 or x.find('Knee') != -1, list(result.columns)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.drop(score_columns, axis=1, inplace=True)\n",
    "#result.drop(set(to_drop)-set(score_columns), axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_array = np.array(result)"
   ]
  },
  {
   "source": [
    "### Preparing data for feeding"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_indices = {}\n",
    "\n",
    "#remember row indices by class\n",
    "for class_id, _class in enumerate(classes):\n",
    "    class_indices[_class] = (np.argwhere(result_array[:, -1] == class_id)).flatten().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_id_indices = []\n",
    "\n",
    "#remember row indices by video_id\n",
    "for x in sorted(set(result_array[:, 0])):\n",
    "    video_id_indices.append((np.argwhere(result_array[:, 0] == x)).flatten().tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_array = np.delete(result_array, 0, 1)\n",
    "result_array = np.delete(result_array, 1, 1)\n",
    "result_array = np.delete(result_array, 2, 1)\n",
    "\n",
    "targets = result_array[:, -1]\n",
    "result_array = np.delete(result_array, -1, 1)\n",
    "n_features = result_array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = [result_array[i] for i in video_id_indices]\n",
    "y = [targets[i] for i in video_id_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = [int(np.amax(y[i])) for i in range(len(y))] #reduce y shape to (200, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "def split(X, y, test_size=0.1):\n",
    "    assert len(X) == len(y)\n",
    "    \n",
    "    y_arr = np.array(y)\n",
    "    onehot = np.zeros((y_arr.size, y_arr.max()+1))\n",
    "    onehot[np.arange(y_arr.size),y_arr] = 1\n",
    "    \n",
    "    shuffled = np.random.permutation(list(range(len(X))))\n",
    "    split_at = int(len(shuffled) * test_size)\n",
    "    \n",
    "    X = itemgetter(*shuffled)(X)\n",
    "    y = itemgetter(*shuffled)(onehot)\n",
    "    \n",
    "    train_X = X[split_at:]\n",
    "    train_y = y[split_at:]\n",
    "    \n",
    "    test_X = X[:split_at]\n",
    "    test_y = y[:split_at]\n",
    "\n",
    "    return (train_X, test_X, train_y, test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X, test_X, train_y, test_y = split(x, y)"
   ]
  },
  {
   "source": [
    "# Model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 100\n",
    "\n",
    "def gen_batch(X, y):\n",
    "    assert len(x) == len(y)\n",
    "\n",
    "    for _ in range(epochs):\n",
    "        for i in range(len(X)):\n",
    "            yield np.array([X[i]]), np.atleast_1d([y[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_batch = gen_batch(train_X, train_y)\n",
    "test_batch = gen_batch(test_X, test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "    layers.Input(shape=[None, n_features], dtype=tf.float64),\n",
    "    layers.LSTM(50, input_shape=(None, n_features), return_sequences=True),\n",
    "    layers.GlobalAveragePooling1D(),\n",
    "    layers.Dense(70, activation='relu'),\n",
    "    layers.Dense(50, activation='relu',kernel_regularizer=tf.keras.regularizers.L1()),\n",
    "    layers.Dense(len(classes), activation='softmax')\n",
    "])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=tf.keras.optimizers.SGD(), loss=tf.keras.losses.CategoricalCrossentropy(from_logits=False), metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = model.fit(train_batch, verbose=1, epochs=100, validation_data=test_batch, steps_per_epoch=180, validation_steps=20)"
   ]
  }
 ]
}