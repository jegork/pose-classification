{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python395jvsc74a57bd0767d51c1340bd893661ea55ea3124f6de3c7a262a8b4abca0554b478b1e2ff90",
   "display_name": "Python 3.9.5 64-bit"
  },
  "metadata": {
   "interpreter": {
    "hash": "767d51c1340bd893661ea55ea3124f6de3c7a262a8b4abca0554b478b1e2ff90"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpu_devices = tf.config.experimental.list_physical_devices(\"GPU\")\n",
    "for device in gpu_devices:\n",
    "    tf.config.experimental.set_memory_growth(device, True)"
   ]
  },
  {
   "source": [
    "# Data loading"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = ['hug', 'kiss', 'highfive', 'handshake']\n",
    "csv_list = os.listdir('out/handshake')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = ['hug', 'kiss', 'highfive', 'handshake']\n",
    "class_videos = {}\n",
    "total = []\n",
    "\n",
    "factorize_classes = {_class: key+1 for (_class, key) in zip(classes, range(len(classes)))}\n",
    "\n",
    "idx = 0\n",
    "\n",
    "for _class in classes:\n",
    "    if os.path.isdir('out/'+_class):\n",
    "        class_videos[_class] = os.listdir('out/'+_class)\n",
    "\n",
    "for _class, files in class_videos.items():\n",
    "    for file in files:\n",
    "        if os.path.isfile(\"out/handshake/\"+file):\n",
    "            if file.split('.')[-1] == 'csv':\n",
    "                csv = pd.read_csv(\"out/handshake/\"+file)\n",
    "                \n",
    "                file_id = int(file.split('.')[0].lstrip('0'))\n",
    "                video_number = pd.DataFrame({'_id': idx, 'video':[file_id]*csv.shape[0]})\n",
    "\n",
    "                csv['result'] = factorize_classes[_class]\n",
    "                total.append(pd.concat([video_number, csv], axis=1))\n",
    "                idx += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = pd.concat(total, ignore_index=True)"
   ]
  },
  {
   "source": [
    "# Data preprocessing"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_columns = list(filter(lambda x: x.endswith('score'), list(result.columns)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = result.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.iloc[:, 4:-1] = result.iloc[:, 4:-1].replace(0, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(result.iloc[:, 4:-1] == 0).sum().sum() / result.iloc[:, 4:-1].size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Proportion of NAs cells in dataset: ' + str((result.iloc[:, 4:-1] == -1).sum().sum() / result.iloc[:, 4:-1].size))\n",
    "print('Proportion of NAs rows in dataset: ' + str((result.iloc[:, 4:-1] == -1).sum(1).count() / result.iloc[:, 4:-1].shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(result[score_columns] == -1).sum().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# columns with parts of lower body which might poorly contribute to prediction of interactions\n",
    "to_drop = list(filter(lambda x: x.find('Ankle') != -1 or x.find('Hip') != -1 or x.find('Knee') != -1, list(result.columns)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.drop(score_columns, axis=1, inplace=True)\n",
    "#result.drop(set(to_drop)-set(score_columns), axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_array = np.array(result)"
   ]
  },
  {
   "source": [
    "### Preparing data for feeding"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_indices = {}\n",
    "\n",
    "#remember row indices by class\n",
    "for class_id, _class in enumerate(classes):\n",
    "    class_indices[_class] = (np.argwhere(result_array[:, -1] == class_id)).flatten().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_id_indices = []\n",
    "\n",
    "#remember row indices by video_id\n",
    "for x in sorted(set(result_array[:, 0])):\n",
    "    video_id_indices.append((np.argwhere(result_array[:, 0] == x)).flatten().tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_array = np.delete(result_array, 0, 1)\n",
    "result_array = np.delete(result_array, 0, 1)\n",
    "result_array = np.delete(result_array, 0, 1)\n",
    "result_array = np.delete(result_array, 0, 1)\n",
    "\n",
    "result_array = result_array / 250\n",
    "n_features = result_array.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = [result_array[i] for i in video_id_indices]\n",
    "y = [targets[i] for i in video_id_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = [int(np.amax(y[i])) for i in range(len(y))] #reduce y shape to (200, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "def split(X, y, test_size=0.1):\n",
    "    assert len(X) == len(y)\n",
    "    \n",
    "    y_arr = np.array(y)\n",
    "    onehot = np.zeros((y_arr.size, y_arr.max()))\n",
    "    onehot[np.arange(y_arr.size),y_arr-1] = 1\n",
    "    \n",
    "    shuffled = np.random.permutation(list(range(len(X))))\n",
    "    split_at = int(len(shuffled) * test_size)\n",
    "    \n",
    "    X = itemgetter(*shuffled)(X)\n",
    "    y = itemgetter(*shuffled)(onehot)\n",
    "    \n",
    "    train_X = X[split_at:]\n",
    "    train_y = y[split_at:]\n",
    "    \n",
    "    test_X = X[:split_at]\n",
    "    test_y = y[:split_at]\n",
    "\n",
    "    return (train_X, test_X, train_y, test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X, test_X, train_y, test_y = split(x, y)"
   ]
  },
  {
   "source": [
    "# Model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 50\n",
    "\n",
    "def gen_batch(X, y):\n",
    "    assert len(X) == len(y)\n",
    "\n",
    "    for _ in range(epochs):\n",
    "        for i in range(len(X)):\n",
    "            yield np.array([X[i]]), np.atleast_1d([y[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_batch = gen_batch(train_X, train_y)\n",
    "test_batch = gen_batch(test_X, test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Input(shape=[None, n_features], dtype=tf.float64))\n",
    "model.add(Bidirectional(LSTM(100, return_sequences=True, )))\n",
    "model.add(GlobalAveragePooling1D())\n",
    "model.add(Dense(200, activation='tanh'))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(4, activation='softmax'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=tf.keras.optimizers.Adam(0.0001), loss=tf.keras.losses.CategoricalCrossentropy(), metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = model.fit(\n",
    "    gen_batch(train_X, train_y), \n",
    "    epochs=30, \n",
    "    validation_data=gen_batch(test_X, test_y), \n",
    "    steps_per_epoch=180, \n",
    "    validation_steps=20\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(16,4))\n",
    "\n",
    "ax[0].plot(h.history['loss'], label='Training')\n",
    "ax[0].plot(h.history['val_loss'], label='Test')\n",
    "ax[0].set_title('Loss')\n",
    "ax[0].set_xlabel('Epochs')\n",
    "ax[0].set_ylabel('Categorical Crossentropy')\n",
    "ax[0].legend()\n",
    "\n",
    "ax[1].plot(h.history['accuracy'], label='Training')\n",
    "ax[1].plot(h.history['val_accuracy'], label='Test')\n",
    "ax[1].set_title('Accuracy')\n",
    "ax[1].set_xlabel('Epochs')\n",
    "ax[1].set_ylabel('Accuracy')\n",
    "ax[1].legend()\n",
    "\n",
    "fig.savefig('./graphs/openpose/openpose_loss_accuracy.png', bbox_inches='tight', pad_inches=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(gen_batch(test_X, test_y), steps=20)\n",
    "y_pred = np.argmax(y_pred, axis=1)\n",
    "y_true = list(map(lambda x: np.argmax(x), test_y))\n",
    "\n",
    "conf_m = sns.heatmap(tf.math.confusion_matrix(y_true, y_pred), annot=True, xticklabels=classes, yticklabels=classes)\n",
    "conf_m.set(xlabel='Predicted labels', ylabel='True labels')\n",
    "conf_m.figure.savefig('./graphs/openpose/openpose_confusion_matrix.png', dpi=150, bbox_inches = \"tight\")"
   ]
  }
 ]
}